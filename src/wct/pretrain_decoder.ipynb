{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e:\\pyenv\\GTCC\\KPG-RL\\HUST-CV-Neural-Style-Transfer\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if os.path.basename(os.getcwd())!=\"HUST-CV-Neural-Style-Transfer\":\n",
    "    %cd ../../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import time\n",
    "from PIL import Image \n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "LEARNING_RATE = 1e-4\n",
    "BATCH_SIZE = 4\n",
    "NUM_EPOCHS = 2\n",
    "IMAGE_SIZE = 256\n",
    "\n",
    "DATASET_PATH = \"./datasets/coco\" # This is now the folder with images directly inside\n",
    "WEIGHTS_DIR = \"models\"\n",
    "os.makedirs(WEIGHTS_DIR, exist_ok=True)\n",
    "\n",
    "LAMBDA_PIXEL = 1.0\n",
    "LAMBDA_FEATURE = 1.0\n",
    "\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg19 = models.vgg19(weights=models.VGG19_Weights.IMAGENET1K_V1).features\n",
    "vgg19.to(DEVICE)\n",
    "for param in vgg19.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "vgg_layer_indices = {\n",
    "    'relu1_1': 1, 'relu2_1': 6, 'relu3_1': 11, 'relu4_1': 20, 'relu5_1': 29\n",
    "}\n",
    "vgg_feature_layers = nn.ModuleList([vgg19[i] for i in range(max(vgg_layer_indices.values()) + 1)])\n",
    "\n",
    "def get_vgg_features(image, target_layer_name):\n",
    "    target_index = vgg_layer_indices[target_layer_name]\n",
    "    features = image\n",
    "    for i, layer in enumerate(vgg_feature_layers):\n",
    "        features = layer(features)\n",
    "        if i == target_index:\n",
    "            return features\n",
    "    raise ValueError(f\"Target layer {target_layer_name} not reached.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_block(in_channels, out_channels):\n",
    "     return nn.Sequential(\n",
    "        nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size=3),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, level):\n",
    "        super().__init__()\n",
    "        self.level = level\n",
    "        layers = []\n",
    "        if level == 5:\n",
    "            layers.extend([\n",
    "                decoder_block(512, 512),\n",
    "                nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "                decoder_block(512, 512), decoder_block(512, 512), decoder_block(512, 512),\n",
    "            ])\n",
    "        if level >= 4:\n",
    "            in_ch = 512 if level == 4 else 512\n",
    "            layers.extend([\n",
    "                decoder_block(in_ch, 512),\n",
    "                nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "                decoder_block(512, 256), decoder_block(256, 256), decoder_block(256, 256), decoder_block(256, 256),\n",
    "            ])\n",
    "        if level >= 3:\n",
    "            in_ch = 256 if level == 3 else 256\n",
    "            layers.extend([\n",
    "                decoder_block(in_ch, 256),\n",
    "                nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "                decoder_block(256, 128), decoder_block(128, 128),\n",
    "            ])\n",
    "        if level >= 2:\n",
    "            in_ch = 128 if level == 2 else 128\n",
    "            layers.extend([\n",
    "                 decoder_block(in_ch, 128),\n",
    "                 nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "                 decoder_block(128, 64),\n",
    "            ])\n",
    "        if level >= 1:\n",
    "            in_ch = 64 if level == 1 else 64\n",
    "            layers.extend([\n",
    "                decoder_block(in_ch, 64),\n",
    "                nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "                nn.Conv2d(64, 3, kernel_size=3)\n",
    "            ])\n",
    "        self.decoder = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, features):\n",
    "        return self.decoder(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoImageDataset(Dataset):\n",
    "    def __init__(self, image_dir, transform=None, supported_extensions=('.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.gif')):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.supported_extensions = supported_extensions\n",
    "\n",
    "        if not os.path.isdir(image_dir):\n",
    "            raise FileNotFoundError(f\"Directory not found: {image_dir}\")\n",
    "\n",
    "        self.image_paths = [\n",
    "            os.path.join(image_dir, fname)\n",
    "            for fname in os.listdir(image_dir)\n",
    "            if fname.lower().endswith(supported_extensions) and os.path.isfile(os.path.join(image_dir, fname))\n",
    "        ]\n",
    "\n",
    "        if not self.image_paths:\n",
    "            print(f\"Warning: No images with supported extensions {supported_extensions} found in {image_dir}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        try:\n",
    "            # Open image using PIL\n",
    "            image = Image.open(img_path).convert('RGB') # Ensure image is RGB\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not load image {img_path}. Error: {e}\")\n",
    "            # Return a dummy tensor or skip? For training, might be better to skip or handle.\n",
    "            # Here, we'll return a placeholder, but filtering corrupted files beforehand is ideal.\n",
    "            # Or, you could re-raise the exception if you want the DataLoader to potentially skip.\n",
    "            return torch.zeros((3, IMAGE_SIZE, IMAGE_SIZE)) # Adjust size if needed\n",
    "\n",
    "        # Apply transformations if provided\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Custom dataset for this task doesn't need a label, so we return only the image.\n",
    "        # DataLoader will handle batching.\n",
    "        return image\n",
    "\n",
    "\n",
    "# -- Data Loading Setup --\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(IMAGE_SIZE),\n",
    "    transforms.CenterCrop(IMAGE_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "])\n",
    "\n",
    "\n",
    "dataset = CocoImageDataset(DATASET_PATH, transform=transform)\n",
    "if len(dataset) == 0:\n",
    "    raise ValueError(\"Dataset is empty. Check path and image extensions.\")\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "print(f\"Found {len(dataset)} images in {DATASET_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Found 50 images in ./datasets/dataset\n",
      "\n",
      "--- Training Decoder for relu1_1 (Level 1) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: 100%|██████████| 13/13 [00:05<00:00,  2.29it/s, PixLoss=1.0998, FeatLoss=0.1317, TotalLoss=1.2315]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 Summary:\n",
      "  Avg Pixel Loss: 1.0822\n",
      "  Avg Feature Loss: 0.2523\n",
      "  Avg Total Loss: 1.3345\n",
      "  Time: 5.93s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2: 100%|██████████| 13/13 [00:04<00:00,  3.06it/s, PixLoss=0.9007, FeatLoss=0.2427, TotalLoss=1.1434]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 Summary:\n",
      "  Avg Pixel Loss: 0.9894\n",
      "  Avg Feature Loss: 0.1894\n",
      "  Avg Total Loss: 1.1788\n",
      "  Time: 10.44s\n",
      "Saved decoder weights for relu1_1 to decoder_weights_wct\\decoder_relu1_1.pth\n",
      "\n",
      "--- Training Decoder for relu2_1 (Level 2) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: 100%|██████████| 13/13 [00:04<00:00,  3.24it/s, PixLoss=0.9379, FeatLoss=1.1128, TotalLoss=2.0507]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 Summary:\n",
      "  Avg Pixel Loss: 1.0653\n",
      "  Avg Feature Loss: 1.8208\n",
      "  Avg Total Loss: 2.8861\n",
      "  Time: 4.22s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2: 100%|██████████| 13/13 [00:03<00:00,  3.30it/s, PixLoss=0.8118, FeatLoss=1.2275, TotalLoss=2.0393]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 Summary:\n",
      "  Avg Pixel Loss: 0.9640\n",
      "  Avg Feature Loss: 1.2324\n",
      "  Avg Total Loss: 2.1963\n",
      "  Time: 8.37s\n",
      "Saved decoder weights for relu2_1 to decoder_weights_wct\\decoder_relu2_1.pth\n",
      "\n",
      "--- Training Decoder for relu3_1 (Level 3) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: 100%|██████████| 13/13 [00:04<00:00,  2.75it/s, PixLoss=0.9170, FeatLoss=8.0694, TotalLoss=8.9864] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 Summary:\n",
      "  Avg Pixel Loss: 1.0768\n",
      "  Avg Feature Loss: 8.7448\n",
      "  Avg Total Loss: 9.8216\n",
      "  Time: 5.07s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2: 100%|██████████| 13/13 [00:04<00:00,  2.86it/s, PixLoss=0.9345, FeatLoss=3.3505, TotalLoss=4.2851] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 Summary:\n",
      "  Avg Pixel Loss: 0.9834\n",
      "  Avg Feature Loss: 6.7129\n",
      "  Avg Total Loss: 7.6963\n",
      "  Time: 9.90s\n",
      "Saved decoder weights for relu3_1 to decoder_weights_wct\\decoder_relu3_1.pth\n",
      "\n",
      "--- Training Decoder for relu4_1 (Level 4) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2:  77%|███████▋  | 10/13 [00:04<00:01,  2.12it/s, PixLoss=1.0252, FeatLoss=20.3209, TotalLoss=21.3461]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 215\u001b[0m\n\u001b[0;32m    212\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    214\u001b[0m \u001b[38;5;66;03m# --- Logging ---\u001b[39;00m\n\u001b[1;32m--> 215\u001b[0m epoch_pixel_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss_p\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m epoch_feature_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_f\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m    217\u001b[0m epoch_total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m total_loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pixel_loss_fn = nn.L1Loss().to(DEVICE)\n",
    "feature_loss_fn = nn.MSELoss().to(DEVICE)\n",
    "\n",
    "vgg_feature_layers.eval()\n",
    "\n",
    "target_layers = ['relu1_1', 'relu2_1', 'relu3_1', 'relu4_1', 'relu5_1']\n",
    "\n",
    "for level, layer_name in enumerate(target_layers, 1):\n",
    "    print(f\"\\n--- Training Decoder for {layer_name} (Level {level}) ---\")\n",
    "\n",
    "    decoder = Decoder(level).to(DEVICE)\n",
    "    optimizer = optim.Adam(decoder.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    total_steps = len(dataloader)\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        epoch_pixel_loss = 0.0\n",
    "        epoch_feature_loss = 0.0\n",
    "        epoch_total_loss = 0.0\n",
    "        decoder.train()\n",
    "\n",
    "        pbar = tqdm(enumerate(dataloader), total=total_steps, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "        \n",
    "        for i, images in pbar: \n",
    "            images = images.to(DEVICE)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                target_features = get_vgg_features(images, layer_name)\n",
    "\n",
    "            reconstructed_images = decoder(target_features)\n",
    "\n",
    "            loss_p = pixel_loss_fn(reconstructed_images, images)\n",
    "\n",
    "            recon_features = get_vgg_features(reconstructed_images, layer_name)\n",
    "            loss_f = feature_loss_fn(recon_features, target_features.detach())\n",
    "\n",
    "            total_loss = LAMBDA_PIXEL * loss_p + LAMBDA_FEATURE * loss_f\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_pixel_loss += loss_p.item()\n",
    "            epoch_feature_loss += loss_f.item()\n",
    "            epoch_total_loss += total_loss.item()\n",
    "\n",
    "            pbar.set_postfix({\n",
    "                'PixLoss': f\"{loss_p.item():.4f}\",\n",
    "                'FeatLoss': f\"{loss_f.item():.4f}\",\n",
    "                'TotalLoss': f\"{total_loss.item():.4f}\"\n",
    "            })\n",
    "\n",
    "        avg_pixel_loss = epoch_pixel_loss / total_steps\n",
    "        avg_feature_loss = epoch_feature_loss / total_steps\n",
    "        avg_total_loss = epoch_total_loss / total_steps\n",
    "        epoch_time = time.time() - start_time\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS} Summary:\")\n",
    "        print(f\"  Avg Pixel Loss: {avg_pixel_loss:.4f}\")\n",
    "        print(f\"  Avg Feature Loss: {avg_feature_loss:.4f}\")\n",
    "        print(f\"  Avg Total Loss: {avg_total_loss:.4f}\")\n",
    "        print(f\"  Time: {epoch_time:.2f}s\")\n",
    "\n",
    "    decoder_save_path = os.path.join(WEIGHTS_DIR, f\"decoder_relu{level}_1.pth\")\n",
    "    torch.save(decoder.state_dict(), decoder_save_path)\n",
    "    print(f\"Saved decoder weights for {layer_name} to {decoder_save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
