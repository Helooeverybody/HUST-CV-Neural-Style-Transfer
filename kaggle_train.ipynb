{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbe4eb85",
   "metadata": {
    "papermill": {
     "duration": 0.003974,
     "end_time": "2025-05-13T15:45:11.427750",
     "exception": false,
     "start_time": "2025-05-13T15:45:11.423776",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f14db99",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T15:45:11.436070Z",
     "iopub.status.busy": "2025-05-13T15:45:11.435809Z",
     "iopub.status.idle": "2025-05-13T15:45:21.602231Z",
     "shell.execute_reply": "2025-05-13T15:45:21.601535Z"
    },
    "papermill": {
     "duration": 10.172244,
     "end_time": "2025-05-13T15:45:21.603908",
     "exception": false,
     "start_time": "2025-05-13T15:45:11.431664",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dataset.py\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import glob\n",
    "import torch\n",
    "\n",
    "\n",
    "class ImageFolderDataset(Dataset):\n",
    "    \"\"\"Dataset for loading images from a folder.\"\"\"\n",
    "\n",
    "    def __init__(self, img_dir, img_size=256, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.img_paths = (\n",
    "            glob.glob(os.path.join(img_dir, \"*.jpg\"))\n",
    "            + glob.glob(os.path.join(img_dir, \"*.png\"))\n",
    "            + glob.glob(os.path.join(img_dir, \"*.jpeg\"))\n",
    "        )\n",
    "        self.img_size = img_size\n",
    "\n",
    "        if transform is None:\n",
    "            self.transform = transforms.Compose(\n",
    "                [\n",
    "                    transforms.Resize((img_size, img_size)),\n",
    "                    transforms.ToTensor(),\n",
    "                    # Keep images in [0, 1] range for general use, VGG preprocessing applied later\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_paths[idx]\n",
    "        try:\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            img = self.transform(img)\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                f\"Warning: Could not load image {img_path}. Error: {e}. Returning None.\"\n",
    "            )\n",
    "            # You might want to return a placeholder or skip this index in the DataLoader collate_fn\n",
    "            return None  # Handle this in the DataLoader's collate_fn\n",
    "        return img\n",
    "\n",
    "    def insert_additional_folder(self, additional_folder):\n",
    "        \"\"\"Insert additional folder to the dataset.\"\"\"\n",
    "        additional_paths = (\n",
    "            glob.glob(os.path.join(additional_folder, \"*.jpg\"))\n",
    "            + glob.glob(os.path.join(additional_folder, \"*.png\"))\n",
    "            + glob.glob(os.path.join(additional_folder, \"*.jpeg\"))\n",
    "        )\n",
    "        self.img_paths.extend(additional_paths)\n",
    "\n",
    "    def shuffle(self):\n",
    "        \"\"\"Shuffle the dataset.\"\"\"\n",
    "        import random\n",
    "\n",
    "        random.shuffle(self.img_paths)\n",
    "\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    \"\"\"Collate function that filters out None values.\"\"\"\n",
    "    batch = list(filter(lambda x: x is not None, batch))\n",
    "    if not batch:\n",
    "        return None  # Return None if the whole batch failed\n",
    "    return torch.utils.data.dataloader.default_collate(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7afee7",
   "metadata": {
    "papermill": {
     "duration": 0.004457,
     "end_time": "2025-05-13T15:45:21.613553",
     "exception": false,
     "start_time": "2025-05-13T15:45:21.609096",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9cc3bd5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T15:45:21.623481Z",
     "iopub.status.busy": "2025-05-13T15:45:21.623182Z",
     "iopub.status.idle": "2025-05-13T15:45:21.630060Z",
     "shell.execute_reply": "2025-05-13T15:45:21.629221Z"
    },
    "papermill": {
     "duration": 0.013306,
     "end_time": "2025-05-13T15:45:21.631591",
     "exception": false,
     "start_time": "2025-05-13T15:45:21.618285",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# vgg.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import vgg19, VGG19_Weights\n",
    "\n",
    "VGG19_LAYER_MAP = {\n",
    "    \"relu1_1\": 1,\n",
    "    \"relu2_1\": 6,\n",
    "    \"relu3_1\": 11,  # Layer used\n",
    "    \"relu4_1\": 20,\n",
    "    \"relu5_1\": 29,\n",
    "}\n",
    "\n",
    "\n",
    "class Vgg19FeatureExtractor(nn.Module):\n",
    "    def __init__(self, layers_to_extract, weights=VGG19_Weights.DEFAULT):\n",
    "        \"\"\"\n",
    "        Initializes the VGG-19 feature extractor.\n",
    "\n",
    "        Args:\n",
    "            layers_to_extract (list): List of layer names (e.g., ['relu3_1'])\n",
    "                                     from which to extract features.\n",
    "            weights (VGG19_Weights): Pretrained weights to use.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.layers_to_extract = sorted(\n",
    "            [VGG19_LAYER_MAP[name] for name in layers_to_extract]\n",
    "        )\n",
    "        self.last_layer_index = self.layers_to_extract[-1]\n",
    "\n",
    "        vgg = vgg19(weights=weights).features\n",
    "        self.model = nn.Sequential(*[vgg[i] for i in range(self.last_layer_index + 1)])\n",
    "\n",
    "        # Freeze VGG parameters\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Extracts features from the specified layers.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input image tensor (B x C x H x W).\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary where keys are layer indices and values\n",
    "                  are the corresponding feature maps.\n",
    "        \"\"\"\n",
    "        features = {}\n",
    "        current_layer_idx = 0\n",
    "        for i, layer in enumerate(self.model):\n",
    "            x = layer(x)\n",
    "            if i in self.layers_to_extract:\n",
    "                features[i] = x\n",
    "                current_layer_idx += 1\n",
    "                if current_layer_idx >= len(self.layers_to_extract):\n",
    "                    break  # No need to compute further\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f9b54d",
   "metadata": {
    "papermill": {
     "duration": 0.004475,
     "end_time": "2025-05-13T15:45:21.640981",
     "exception": false,
     "start_time": "2025-05-13T15:45:21.636506",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Inverse net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2da794dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T15:45:21.649313Z",
     "iopub.status.busy": "2025-05-13T15:45:21.648924Z",
     "iopub.status.idle": "2025-05-13T15:45:21.656027Z",
     "shell.execute_reply": "2025-05-13T15:45:21.655367Z"
    },
    "papermill": {
     "duration": 0.011679,
     "end_time": "2025-05-13T15:45:21.657268",
     "exception": false,
     "start_time": "2025-05-13T15:45:21.645589",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# inverse_net.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class InverseNetwork(nn.Module):\n",
    "    def __init__(self, input_channels=256):  # Channels for relu3_1 of VGG19\n",
    "        \"\"\"\n",
    "        Initializes the Inverse Network based on Appendix Table A2.\n",
    "        Assumes input is from VGG19's relu3_1 (256 channels).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Input: 1/4 H x 1/4 W x 256 (relu3_1 for 256x256 image)\n",
    "        self.layers = nn.Sequential(\n",
    "            # Conv-InstanceNorm-ReLU Block 1\n",
    "            nn.ConvTranspose2d(input_channels, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),  # Changed to BatchNorm2d for consistency\n",
    "            nn.PReLU(),\n",
    "            # Upsampling + Conv-InstanceNorm-ReLU Block 2\n",
    "            nn.Upsample(scale_factor=2, mode=\"nearest\"),  # To 1/2 H x 1/2 W\n",
    "            nn.ConvTranspose2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),  # Changed to BatchNorm2d for consistency\n",
    "            nn.PReLU(),\n",
    "            # Conv-InstanceNorm-ReLU Block 3\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),  # Changed to BatchNorm2d for consistency\n",
    "            nn.PReLU(),\n",
    "            # Upsampling + Conv-InstanceNorm-ReLU Block 4\n",
    "            nn.Upsample(scale_factor=2, mode=\"nearest\"),  # To H x W\n",
    "            nn.ConvTranspose2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),  # Changed to BatchNorm2d for consistency\n",
    "            nn.PReLU(),\n",
    "            # Output Convolution\n",
    "            nn.ConvTranspose2d(64, 3, kernel_size=3, stride=1, padding=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Initialize weights of the network.\n",
    "        \"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.ConvTranspose2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97243379",
   "metadata": {
    "papermill": {
     "duration": 0.004366,
     "end_time": "2025-05-13T15:45:21.666117",
     "exception": false,
     "start_time": "2025-05-13T15:45:21.661751",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Style swap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c6f80c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T15:45:21.673908Z",
     "iopub.status.busy": "2025-05-13T15:45:21.673710Z",
     "iopub.status.idle": "2025-05-13T15:45:21.685263Z",
     "shell.execute_reply": "2025-05-13T15:45:21.684602Z"
    },
    "papermill": {
     "duration": 0.016285,
     "end_time": "2025-05-13T15:45:21.686629",
     "exception": false,
     "start_time": "2025-05-13T15:45:21.670344",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# style_swap.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def extract_patches(feature_map, patch_size=3, stride=1):\n",
    "    \"\"\"Extracts patches from a feature map.\n",
    "\n",
    "    Args:\n",
    "        feature_map (torch.Tensor): Input feature map (B x C x H x W).\n",
    "        patch_size (int): Size of the square patches.\n",
    "        stride (int): Stride for patch extraction.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Extracted patches (B * n_patches_h * n_patches_w, C, patch_size, patch_size).\n",
    "    \"\"\"\n",
    "    B, C, H, W = feature_map.shape\n",
    "    # Use unfold to extract patches\n",
    "    # unfold(dimension, size, step)\n",
    "    patches = F.unfold(feature_map, kernel_size=patch_size, stride=stride)\n",
    "    # patches shape: (B, C * patch_size * patch_size, n_patches_h * n_patches_w)\n",
    "    patches = patches.permute(0, 2, 1).contiguous()\n",
    "    # patches shape: (B, n_patches_h * n_patches_w, C * patch_size * patch_size)\n",
    "    n_patches_total = patches.shape[1]\n",
    "    patches = patches.view(B * n_patches_total, C, patch_size, patch_size)\n",
    "    return patches\n",
    "\n",
    "\n",
    "def style_swap_op(\n",
    "    content_features, style_features, patch_size=3, stride=1, eps=1e-8, verbose=False\n",
    "):\n",
    "    \"\"\"Performs the Style Swap operation.\n",
    "\n",
    "    Args:\n",
    "        content_features (torch.Tensor): Content feature map (B x C x H x W).\n",
    "        style_features (torch.Tensor): Style feature map (B x C x H' x W').\n",
    "        patch_size (int): Size of the patches.\n",
    "        stride (int): Stride for patch matching and reconstruction.\n",
    "        eps (float): Epsilon for numerical stability (e.g., in normalization).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The resulting feature map after style swap (B x C x H x W).\n",
    "    \"\"\"\n",
    "    device = content_features.device\n",
    "    B_c, C_c, H_c, W_c = content_features.shape\n",
    "    B_s, C_s, H_s, W_s = style_features.shape\n",
    "\n",
    "    assert (\n",
    "        C_c == C_s\n",
    "    ), \"Content and Style features must have the same number of channels.\"\n",
    "    C = C_c\n",
    "\n",
    "    # 1. Extract patches from style features\n",
    "    # style_patches shape: (B_s * N_s, C, patch_size, patch_size) where N_s is num style patches\n",
    "    if verbose:\n",
    "        print(f\"Extracting patches from style features...\")\n",
    "        print(f\"Style features shape: {style_features.shape}\")\n",
    "        print(f\"Content features: \\n{content_features}\")\n",
    "        print(f\"Style features shape: {style_features.shape}\")\n",
    "        print(f\"Style features: \\n{style_features}\")\n",
    "\n",
    "    style_patches = extract_patches(style_features, patch_size, stride)\n",
    "    if verbose:\n",
    "        print(f\"Extracted style patches shape: {style_patches.shape}\")\n",
    "        print(f\"Style patches: \\n{style_patches}\")\n",
    "\n",
    "    # 2. Normalize style patches (for correlation calculation)\n",
    "    # norm shape: (B_s * N_s, 1, 1, 1)\n",
    "    style_patches_norm = torch.sqrt(\n",
    "        torch.sum(style_patches**2, dim=(1, 2, 3), keepdim=True)\n",
    "    )\n",
    "    # Avoid division by zero for zero-patches\n",
    "    style_patches_normalized = style_patches / (style_patches_norm + eps)\n",
    "\n",
    "    # 3. Compute correlation using convolution\n",
    "    # Use style patches as convolution filters\n",
    "    # conv_filters shape: (n_style_patches, C, patch_size, patch_size)\n",
    "    conv_filters = style_patches_normalized.to(device)\n",
    "    # content_features shape: (B_c, C, H_c, W_c)\n",
    "    # correlation_maps shape: (B_c, n_style_patches, H_out, W_out)\n",
    "    correlation_maps = F.conv2d(\n",
    "        content_features, conv_filters, stride=stride, padding=0\n",
    "    )  # Using 0 padding\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Correlation maps shape: {correlation_maps.shape}\")\n",
    "        print(f\"Correlation maps: \\n{correlation_maps}\")\n",
    "\n",
    "    # 4. Find the best matching style patch for each content patch (Channel-wise Argmax)\n",
    "    # best_match_indices shape: (B_c, H_out, W_out)\n",
    "    best_match_indices = torch.argmax(correlation_maps, dim=1)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Best match indices shape: {best_match_indices.shape}\")\n",
    "        print(f\"Best match indices: \\n{best_match_indices}\")\n",
    "\n",
    "    # 5. Create one-hot selection map\n",
    "    # one_hot_map shape: (B_c, n_style_patches, H_out, W_out)\n",
    "    H_out, W_out = best_match_indices.shape[1], best_match_indices.shape[2]\n",
    "    one_hot_map = torch.zeros_like(correlation_maps, device=device)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"One-hot map shape before scatter: {one_hot_map.shape}\")\n",
    "        print(f\"One-hot map: \\n{one_hot_map}\")\n",
    "    # Use scatter_ to place 1s at the argmax indices\n",
    "    # scatter_(dimension, index_tensor, value)\n",
    "    # index needs to be same shape as output after indexing dim -> add channel dim\n",
    "    one_hot_map.scatter_(1, best_match_indices.unsqueeze(1), 1.0)\n",
    "    if verbose:\n",
    "        print(f\"One-hot map shape after scatter: {one_hot_map.shape}\")\n",
    "        print(f\"One-hot map: \\n{one_hot_map}\")\n",
    "\n",
    "    # 6. Reconstruct using transposed convolution\n",
    "    # Use original (unnormalized) style patches as filters\n",
    "    # recon_filters shape: (n_style_patches, C, patch_size, patch_size)\n",
    "    recon_filters = style_patches.to(device)\n",
    "    # output_padding adjusts output size, often needed if stride > 1\n",
    "    # calculate required output padding if needed, or ensure input sizes work well\n",
    "    # For stride=1, output_padding is typically 0\n",
    "    # swapped_features_sum shape: (B_c, C, H_rec, W_rec) -> should approximate H_c, W_c\n",
    "    swapped_features_sum = F.conv_transpose2d(\n",
    "        one_hot_map, recon_filters, stride=stride, padding=0\n",
    "    )\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Swapped features sum shape: {swapped_features_sum.shape}\")\n",
    "        print(f\"Swapped features sum: \\n{swapped_features_sum}\")\n",
    "\n",
    "    # 7. Normalize for overlapping patches\n",
    "    # Create filters of ones for counting overlaps\n",
    "    # count_filters shape: (1, 1, patch_size, patch_size)\n",
    "    count_filters = torch.ones(1, 1, patch_size, patch_size, device=device)\n",
    "    # Count contributions per pixel\n",
    "    # one_hot_map shape: (B_c, n_style_patches, H_out, W_out)\n",
    "    # Reduce one_hot_map along the patch dimension before counting\n",
    "    # reduced_one_hot shape: (B_c, 1, H_out, W_out)\n",
    "    reduced_one_hot = torch.sum(one_hot_map, dim=1, keepdim=True)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Reduced one-hot map shape: {reduced_one_hot.shape}\")\n",
    "        print(f\"Reduced one-hot map: \\n{reduced_one_hot}\")\n",
    "    # overlap_count shape: (B_c, 1, H_rec, W_rec)\n",
    "    overlap_count = F.conv_transpose2d(\n",
    "        reduced_one_hot, count_filters, stride=stride, padding=0\n",
    "    )\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Overlap count shape: {overlap_count.shape}\")\n",
    "        print(f\"Overlap count: \\n{overlap_count}\")\n",
    "\n",
    "    # Average the contributions\n",
    "    # Add eps to avoid division by zero where there's no patch contribution\n",
    "    swapped_features_avg = swapped_features_sum / (overlap_count + eps)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Swapped features average shape: {swapped_features_avg.shape}\")\n",
    "        print(f\"Swapped features average: \\n{swapped_features_avg}\")\n",
    "\n",
    "    return swapped_features_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6841a4",
   "metadata": {
    "papermill": {
     "duration": 0.004288,
     "end_time": "2025-05-13T15:45:21.695659",
     "exception": false,
     "start_time": "2025-05-13T15:45:21.691371",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "744c9755",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T15:45:21.704848Z",
     "iopub.status.busy": "2025-05-13T15:45:21.704478Z",
     "iopub.status.idle": "2025-05-13T15:45:21.709982Z",
     "shell.execute_reply": "2025-05-13T15:45:21.709379Z"
    },
    "papermill": {
     "duration": 0.01081,
     "end_time": "2025-05-13T15:45:21.711069",
     "exception": false,
     "start_time": "2025-05-13T15:45:21.700259",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# loss.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class TVLoss(nn.Module):\n",
    "    \"\"\"Total Variation Loss\"\"\"\n",
    "\n",
    "    def __init__(self, weight=1.0):\n",
    "        super().__init__()\n",
    "        self.weight = weight\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): Input image tensor (B x C x H x W).\n",
    "        Returns:\n",
    "            torch.Tensor: Scalar TV loss value.\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        h_x = x.size(2)\n",
    "        w_x = x.size(3)\n",
    "        count_h = (h_x - 1) * w_x\n",
    "        count_w = h_x * (w_x - 1)\n",
    "        h_tv = torch.pow((x[:, :, 1:, :] - x[:, :, :-1, :]), 2).sum()\n",
    "        w_tv = torch.pow((x[:, :, :, 1:] - x[:, :, :, :-1]), 2).sum()\n",
    "        loss = self.weight * (h_tv / count_h + w_tv / count_w) / batch_size\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e911d12",
   "metadata": {
    "papermill": {
     "duration": 0.002462,
     "end_time": "2025-05-13T15:45:21.716159",
     "exception": false,
     "start_time": "2025-05-13T15:45:21.713697",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1db42da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T15:45:21.722486Z",
     "iopub.status.busy": "2025-05-13T15:45:21.721853Z",
     "iopub.status.idle": "2025-05-13T15:45:21.731520Z",
     "shell.execute_reply": "2025-05-13T15:45:21.730942Z"
    },
    "papermill": {
     "duration": 0.013884,
     "end_time": "2025-05-13T15:45:21.732648",
     "exception": false,
     "start_time": "2025-05-13T15:45:21.718764",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# utils.py\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# VGG preprocessing values\n",
    "VGG_MEAN = [0.485, 0.456, 0.406]\n",
    "VGG_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "\n",
    "def get_device():\n",
    "    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def load_image(image_path, img_size=None):\n",
    "    \"\"\"Loads an image and optionally resizes it.\"\"\"\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    if img_size is not None:\n",
    "        img = img.resize((img_size, img_size), Image.LANCZOS)\n",
    "    return img\n",
    "\n",
    "\n",
    "def preprocess_image(img, device):\n",
    "    \"\"\"Preprocesses a PIL image for VGG input.\"\"\"\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=VGG_MEAN, std=VGG_STD),\n",
    "        ]\n",
    "    )\n",
    "    return transform(img).unsqueeze(0).to(device)\n",
    "\n",
    "\n",
    "def postprocess_image(tensor):\n",
    "    \"\"\"Postprocesses a tensor back to a PIL image.\"\"\"\n",
    "    # Ensure tensor is on CPU\n",
    "    tensor = tensor.squeeze(0).cpu().detach().clone()\n",
    "    # Denormalize\n",
    "    mean = torch.tensor(VGG_MEAN).view(3, 1, 1)\n",
    "    std = torch.tensor(VGG_STD).view(3, 1, 1)\n",
    "    tensor = tensor * std + mean\n",
    "    # Clamp values to [0, 1]\n",
    "    tensor = torch.clamp(tensor, 0, 1)\n",
    "    # Convert to PIL Image\n",
    "    img = transforms.ToPILImage()(tensor)\n",
    "    return img\n",
    "\n",
    "\n",
    "def save_image(pil_img, save_path):\n",
    "    \"\"\"Saves a PIL image.\"\"\"\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    pil_img.save(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d526291",
   "metadata": {
    "papermill": {
     "duration": 0.00369,
     "end_time": "2025-05-13T15:45:21.739191",
     "exception": false,
     "start_time": "2025-05-13T15:45:21.735501",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8da96ca3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T15:45:21.745031Z",
     "iopub.status.busy": "2025-05-13T15:45:21.744850Z",
     "iopub.status.idle": "2025-05-13T15:45:21.749140Z",
     "shell.execute_reply": "2025-05-13T15:45:21.748375Z"
    },
    "papermill": {
     "duration": 0.008397,
     "end_time": "2025-05-13T15:45:21.750217",
     "exception": false,
     "start_time": "2025-05-13T15:45:21.741820",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "CONTENT_DIR = \"/kaggle/input/coco-wikiart-nst-dataset-512-100000/content\"\n",
    "STYLE_DIR = \"/kaggle/input/coco-wikiart-nst-dataset-512-100000/style\"\n",
    "CHECKPOINT_DIR = \"/kaggle/working/checkpoints/\"\n",
    "LAYER = \"relu3_1\"\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 16\n",
    "IMG_SIZE = 512\n",
    "LR = 0.001\n",
    "LR_DECAY = 0.0001\n",
    "LAMBDA_TV = 1e-6\n",
    "LAMBDA_CONTENT = 1.0\n",
    "LAMBDA_PIXEL = 1.4\n",
    "LOSS_TYPE = \"mse\"\n",
    "NUM_WORKERS = 4\n",
    "SAVE_INTERVAL = 5000\n",
    "LOG_INTERVAL = 100\n",
    "MAX_CHECKPOINTS_NUM = 10\n",
    "RESUME = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58535ec9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T15:45:21.756269Z",
     "iopub.status.busy": "2025-05-13T15:45:21.756067Z",
     "iopub.status.idle": "2025-05-13T21:22:38.945498Z",
     "shell.execute_reply": "2025-05-13T21:22:38.944525Z"
    },
    "papermill": {
     "duration": 20237.193995,
     "end_time": "2025-05-13T21:22:38.946686",
     "exception": false,
     "start_time": "2025-05-13T15:45:21.752691",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n",
      "100%|██████████| 548M/548M [00:02<00:00, 230MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's use 2 GPUs!\n",
      "Loading dataset...\n",
      "Starting Pre-training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 6247/6247 [1:07:20<00:00,  1.55it/s, Loss=0.0694, Pixel=0.0046, Content=0.0647, TV=0.0000]\n",
      "Epoch 2/5: 100%|██████████| 6247/6247 [1:07:28<00:00,  1.54it/s, Loss=0.0902, Pixel=0.0060, Content=0.0842, TV=0.0000]\n",
      "Epoch 3/5: 100%|██████████| 6247/6247 [1:07:27<00:00,  1.54it/s, Loss=0.0428, Pixel=0.0029, Content=0.0399, TV=0.0000]\n",
      "Epoch 4/5: 100%|██████████| 6247/6247 [1:07:27<00:00,  1.54it/s, Loss=0.0678, Pixel=0.0045, Content=0.0633, TV=0.0000]\n",
      "Epoch 5/5: 100%|██████████| 6247/6247 [1:07:26<00:00,  1.54it/s, Loss=0.0509, Pixel=0.0034, Content=0.0475, TV=0.0000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pre-training finished. Final model saved to /kaggle/working/checkpoints/invnet_pretrain_final.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# pretrain_invnet.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms  # Needed for VGG preprocessing\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# --- Setup ---\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "device = get_device()\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# VGG Preprocessing Transformation (Applied during training loop)\n",
    "vgg_preprocess = transforms.Normalize(mean=VGG_MEAN, std=VGG_STD)\n",
    "\n",
    "# Target VGG layer index\n",
    "try:\n",
    "    target_layer_idx = VGG19_LAYER_MAP[LAYER]\n",
    "except KeyError:\n",
    "    print(\n",
    "        f\"Error: Invalid VGG layer name '{LAYER}'. Choose from {list(VGG19_LAYER_MAP.keys())}\"\n",
    "    )\n",
    "    exit(1)\n",
    "\n",
    "# Models\n",
    "print(\"Loading models...\")\n",
    "vgg_extractor = Vgg19FeatureExtractor([LAYER]).to(device).eval()\n",
    "# Freeze VGG (already done in class definition)\n",
    "\n",
    "# Determine input channels based on chosen VGG layer\n",
    "if LAYER == \"relu3_1\":\n",
    "    inv_net_channels = 256\n",
    "elif LAYER == \"relu4_1\":\n",
    "    inv_net_channels = 512\n",
    "else:\n",
    "    inv_net_channels = 256  # Default guess\n",
    "\n",
    "base_inverse_net = InverseNetwork(input_channels=inv_net_channels).to(device)\n",
    "base_inverse_net.init_weights()\n",
    "base_inverse_net.to(device)\n",
    "\n",
    "if torch.cuda.is_available() and torch.cuda.device_count() > 1:\n",
    "    print(f\"Let's use {torch.cuda.device_count()} GPUs!\")\n",
    "    inverse_net = nn.DataParallel(base_inverse_net)\n",
    "else:\n",
    "    inverse_net = base_inverse_net # Use the base model directly if not using DataParallel\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(\n",
    "    inverse_net.parameters(), lr=LR, weight_decay=LR_DECAY\n",
    ")\n",
    "\n",
    "# Loss Functions\n",
    "if LOSS_TYPE == \"mse\":\n",
    "    pixel_loss_fn = nn.MSELoss().to(device)\n",
    "else:  # l1\n",
    "    pixel_loss_fn = nn.L1Loss().to(device)\n",
    "\n",
    "tv_loss_fn = TVLoss(weight=LAMBDA_TV).to(device)\n",
    "\n",
    "# Dataset and Dataloader\n",
    "print(\"Loading dataset...\")\n",
    "# Dataset transform expects images in [0, 1] range\n",
    "dataset_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "train_dataset = ImageFolderDataset(\n",
    "    CONTENT_DIR, img_size=IMG_SIZE, transform=dataset_transform\n",
    ")\n",
    "\n",
    "train_dataset.insert_additional_folder(STYLE_DIR)\n",
    "train_dataset.shuffle()  # Shuffle dataset\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    "    collate_fn=custom_collate_fn,\n",
    "    drop_last=True,\n",
    ")  # drop_last helps if filtering causes variable batch sizes\n",
    "\n",
    "# --- Resume training if checkpoint provided ---\n",
    "start_epoch = 0\n",
    "global_step = 0\n",
    "if RESUME:\n",
    "    if os.path.isfile(RESUME):\n",
    "        print(f\"=> Loading checkpoint '{RESUME}'\")\n",
    "        try:\n",
    "            # Use weights_only=False as we saved optimizer state and args\n",
    "            checkpoint = torch.load(\n",
    "                RESUME, map_location=device, weights_only=False\n",
    "            )\n",
    "            start_epoch = checkpoint.get(\n",
    "                \"epoch\", 0\n",
    "            )  # Get epoch, default to 0 if missing\n",
    "            global_step = checkpoint.get(\"global_step\", 0)  # Get step, default to 0\n",
    "            inverse_net.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "            optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "            # Restore learning rate from optimizer state if needed, or keep args.lr\n",
    "            # Example: optimizer.param_groups[0]['lr'] = checkpoint.get('learning_rate', args.lr)\n",
    "            print(\n",
    "                f\"=> Loaded checkpoint '{RESUME}' (epoch {start_epoch}, step {global_step})\"\n",
    "            )\n",
    "        except TypeError as e:\n",
    "            if \"unexpected keyword argument 'weights_only'\" in str(e):\n",
    "                print(\n",
    "                    \"Warning: PyTorch version might be too old for 'weights_only'. Loading without it.\"\n",
    "                )\n",
    "                checkpoint = torch.load(RESUME, map_location=device)\n",
    "                start_epoch = checkpoint.get(\"epoch\", 0)\n",
    "                global_step = checkpoint.get(\"global_step\", 0)\n",
    "                inverse_net.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "                optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "                print(\n",
    "                    f\"=> Loaded checkpoint '{RESUME}' (epoch {start_epoch}, step {global_step})\"\n",
    "                )\n",
    "            else:\n",
    "                print(f\"Error loading checkpoint: {e}\")\n",
    "                exit(1)  # Exit if it's another TypeError\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading checkpoint: {e}\")\n",
    "            # Decide if you want to exit or continue training from scratch\n",
    "            print(\"Could not load checkpoint, starting training from scratch.\")\n",
    "            start_epoch = 0\n",
    "            global_step = 0\n",
    "    else:\n",
    "        print(\n",
    "            f\"=> No checkpoint found at '{RESUME}', starting training from scratch.\"\n",
    "        )\n",
    "\n",
    "        inverse_net.init_weights()  # Initialize weights if starting from scratch\n",
    "\n",
    "# --- Pre-training Loop ---\n",
    "print(\"Starting Pre-training...\")\n",
    "# We resume from the epoch *after* the saved one\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    inverse_net.train()  # Set model to training mode\n",
    "    progress_bar = tqdm(\n",
    "        enumerate(train_loader),\n",
    "        total=len(train_loader),\n",
    "        desc=f\"Epoch {epoch+1}/{EPOCHS}\",\n",
    "    )\n",
    "\n",
    "    for batch_idx, original_images in progress_bar:\n",
    "        # Handle potential None batches from loader/collate_fn\n",
    "        if original_images is None:\n",
    "            # print(f\"Warning: Skipping empty batch at index {batch_idx}.\") # Can be noisy\n",
    "            continue\n",
    "\n",
    "        original_images = original_images.to(device)\n",
    "\n",
    "        # --- Forward Pass ---\n",
    "        images_vgg = vgg_preprocess(original_images)\n",
    "        with torch.no_grad():\n",
    "            features = vgg_extractor(images_vgg)[target_layer_idx]\n",
    "        reconstructed_images = inverse_net(features)\n",
    "\n",
    "        # --- Loss Calculation ---\n",
    "\n",
    "        clamped_reconstructed_images = torch.clamp(reconstructed_images, 0, 1)\n",
    "\n",
    "        reconstructed_images_vgg = vgg_preprocess(clamped_reconstructed_images)\n",
    "\n",
    "        loss_content = LAMBDA_CONTENT * pixel_loss_fn(\n",
    "            reconstructed_images_vgg, images_vgg\n",
    "        )\n",
    "        loss_tv = tv_loss_fn(reconstructed_images)\n",
    "        loss_pixel = LAMBDA_PIXEL * pixel_loss_fn(\n",
    "            clamped_reconstructed_images, original_images\n",
    "        )\n",
    "        total_loss = loss_pixel + loss_tv + loss_content\n",
    "\n",
    "        # --- Backward Pass & Optimization ---\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # --- Logging ---\n",
    "        if global_step % LOG_INTERVAL == 0:\n",
    "            progress_bar.set_postfix(\n",
    "                {\n",
    "                    \"Loss\": f\"{total_loss.item():.4f}\",\n",
    "                    \"Pixel\": f\"{loss_pixel.item():.4f}\",\n",
    "                    \"Content\": f\"{loss_content.item():.4f}\",\n",
    "                    \"TV\": f\"{loss_tv.item():.4f}\",\n",
    "                }\n",
    "            )\n",
    "\n",
    "        # --- Checkpointing ---\n",
    "        # Also save at the end of each epoch? Optional.\n",
    "        if global_step > 0 and global_step % SAVE_INTERVAL == 0:\n",
    "            ckpt_path = os.path.join(\n",
    "                CHECKPOINT_DIR, f\"invnet_pretrain_step_{global_step}.pth\"\n",
    "            )\n",
    "\n",
    "            list_of_checkpoints = sorted(\n",
    "                glob.glob(os.path.join(CHECKPOINT_DIR, \"invnet_pretrain_step_*.pth\"))\n",
    "            )\n",
    "            if len(list_of_checkpoints) > MAX_CHECKPOINTS_NUM:\n",
    "                # Remove the oldest checkpoint\n",
    "                os.remove(list_of_checkpoints[0])\n",
    "                print(f\"Removed oldest checkpoint: {list_of_checkpoints[0]}\")\n",
    "            \n",
    "            torch.save(\n",
    "                {\n",
    "                    \"epoch\": epoch,  # Save the *current* epoch number\n",
    "                    \"global_step\": global_step,\n",
    "                    \"model_state_dict\": inverse_net.state_dict(),\n",
    "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                    \"args\": {\n",
    "                        \"layer\": LAYER,\n",
    "                        \"epochs\": EPOCHS,\n",
    "                        \"batch_size\": BATCH_SIZE,\n",
    "                        \"img_size\": IMG_SIZE,\n",
    "                        \"lr\": LR,\n",
    "                        \"lr_decay\": LR_DECAY,\n",
    "                        \"lambda_tv\": LAMBDA_TV,\n",
    "                        \"lambda_content\": LAMBDA_CONTENT,\n",
    "                        \"lambda_pixel\": LAMBDA_PIXEL,\n",
    "                        \"loss_type\": LOSS_TYPE,\n",
    "                    },\n",
    "                },\n",
    "                ckpt_path,\n",
    "            )\n",
    "\n",
    "            global_step += 1\n",
    "\n",
    "            INSPECT_DIR = \"/kaggle/working/inspect_pretrain/\"\n",
    "            VALIDATE_IMG = \"/kaggle/input/coco-wikiart-nst-dataset-512-100000/content/000000000291.jpg\"\n",
    "\n",
    "            img = Image.open(VALIDATE_IMG).convert(\"RGB\")\n",
    "\n",
    "            transform = transforms.Compose(\n",
    "                [\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=VGG_MEAN, std=VGG_STD),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            img = transform(img).unsqueeze(0).to(device)  # Add batch dimension\n",
    "\n",
    "            with torch.no_grad():\n",
    "                features = vgg_extractor(img)[target_layer_idx]\n",
    "                reconstructed_images = inverse_net(features)\n",
    "                clamped_reconstructed_images = torch.clamp(reconstructed_images, 0, 1)\n",
    "\n",
    "            if clamped_reconstructed_images.ndim == 4:\n",
    "                last_batch_image = clamped_reconstructed_images[\n",
    "                    0\n",
    "                ]  # Get the first image of the last batch\n",
    "            elif clamped_reconstructed_images.ndim == 3:\n",
    "                last_batch_image = clamped_reconstructed_images\n",
    "\n",
    "            os.makedirs(INSPECT_DIR, exist_ok=True)\n",
    "            save_path = os.path.join(INSPECT_DIR, f\"invnet_last_batch_epoch_{epoch+1}.png\")\n",
    "            # Convert to PIL and save\n",
    "            to_pil = transforms.ToPILImage()\n",
    "            last_img_pil = to_pil(last_batch_image.cpu())\n",
    "            last_img_pil.save(save_path)\n",
    "            print(f\"Last batch of generated images saved to {save_path}\")\n",
    "\n",
    "# --- Save Final Model ---\n",
    "final_path = os.path.join(CHECKPOINT_DIR, \"invnet_pretrain_final.pth\")\n",
    "torch.save(\n",
    "    {\n",
    "        \"epoch\": EPOCHS,  # Save final epoch count\n",
    "        \"global_step\": global_step,\n",
    "        \"model_state_dict\": inverse_net.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"args\": {\n",
    "            \"layer\": LAYER,\n",
    "            \"epochs\": EPOCHS,\n",
    "            \"batch_size\": BATCH_SIZE,\n",
    "            \"img_size\": IMG_SIZE,\n",
    "            \"lr\": LR,\n",
    "            \"lr_decay\": LR_DECAY,\n",
    "            \"lambda_tv\": LAMBDA_TV,\n",
    "            \"lambda_content\": LAMBDA_CONTENT,\n",
    "            \"lambda_pixel\": LAMBDA_PIXEL,\n",
    "            \"loss_type\": LOSS_TYPE,\n",
    "        },\n",
    "    },\n",
    "    final_path,\n",
    ")\n",
    "print(f\"\\nPre-training finished. Final model saved to {final_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 1914240,
     "sourceId": 3144104,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 20258.14738,
   "end_time": "2025-05-13T21:22:44.952574",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-13T15:45:06.805194",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
